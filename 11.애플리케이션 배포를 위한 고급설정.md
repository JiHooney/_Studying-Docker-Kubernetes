# 11.1 포드의 자원 사용량 제한

```bash
쿠버네티스와 같은 컨테이너 오케스트레이션 툴의 가장 큰 장점은 여러 대의 서버를 묶어서
리소스 풀로 사용하는 것. 클러스터의 CPU나 메모리 등의 자원이 부족할 때,
필요한 용량만큼의 서버를 동적으로 추가함으로써 수평적으로 확장할 수 있다.
또 중요한 것은 클러스터 내부에서 컴퓨팅 자원 활용률을 늘리는 것이다.

자원활용률은 서버 클러스터에서 자원을 얼마나 효율적으로, 빠짐없이 사용하고 있는지를 의미한다.
쿠버네티스는 컴퓨팅 자원을 컨테이너에 할당하기 위한 여러 기능을 제공한다.

이번 장에서는 포드나 컨테이너에 CPU, 메모리 등의 자원을 할당하는 기본적인 방법을 알아보고,
쿠버네티스 클러스터 자원의 활용률을 높이기 위한 오버커밋 방법을 배운다.
그리고 ResourceQuota와 LimitRange 오브젝트를 배운다.
```

- 컨테이너와 포드의 자원사용량 제한: Limits

```bash
컨테이너의 자원사용량 제한방법은 여러가지있다.
--memory옵션은 사용 가능한 최대 메모리 사용량을 제한하며,
그 외의 옵션들은 CPU 사용량을 제한한다.
--memory
--cpus
--cpu-shares : 서버에CPU가 실제로 몇 개가 있는지 상관없이 할당 비율에 따라서 컨테이너가
	사용할 수 있는 CPU 자원이 결정되는 옵션
--cpu-quota
--cpu-runtime

> docker run -it --name memory_1gb --memory 1g ubuntu:16.04
> docker run -it --name cpu_1_alloc --cpus 1 ubuntu:16.04
> docker run -it --name cpu_shares_example --cpu-shared 1024 ubuntu:16.04

또는 도커 컨테이너를 생성하면 호스트의 모든 CPU, 메모리를 사용할 수 있다.
> docker run -it --name unlimited_blade ubuntu:16.04

쿠버네티스는 기본적으로 도커를 컨테이너 런타임으로 사용하기 때문에 포드를 생성할 때
docker와 동일한 원리로 CPU, 메모리의 최대 사용량을 제한할 수 있다.
포드나 디플로이먼트를 만들 때 아무것도 안적으면 자원 할당량을 제한하지 않지만
이럴 경우 포드의 컨테이너가 노드의 물리자원을 모두 사용할 수 있기 때문에 
자원사용량을 적어주는 것이 좋다.

> vi resource-limit-pod.yaml

apiVersion: v1
kind: Pod
metadata: 
  name: resource-limit-pod
  labels:
    name: resource-limit-pod
spec:
  containers:
  - name: nginx
    image: nginx:latest
  resources:
    limits:
      memory: "256Mi"
      cpu: "1000m"

cpu에서는 1개의 cpu를 뜻하는 1000m(밀리코어)라는 값을 입력한다.
위 yaml을 docker에서는 아래와 같다.
docekr run --memory 256m
docker run --cpus 1

> kubectl apply -f resource-limit-pod.yaml

도커에서는 docker info 명령어로 도커 호스트의 가용 자원을 확인할 수 있다.
쿠버네티스는 아래와 같다.

#아래 명령어로 NODE의 ip확인
> kubectl get pods -o wide

> kubectl describe node $ip

Non-terminated Pods항목에서는 해당 노드에서 실행 중인 포드들의 자원 할당량을 확인할 수 있다.

```

- 컨테이너와 포드의 자원 사용량 제한하기: Requests

```bash
쿠버네티스의 자원 관리에서 Requests는 '적어도 이 만큼의 자원은 컨테이너에게 보장돼야 한다.'
는 것을 의미한다.
Requests는 쿠버네티스에서 자원의 오버커밋(Overcommit)을 가능하게 만드는 기능이기 때문에
정확한 개념을 알고 넘어가는 것이 좋다.

**오버커밋**이란 한정된 컴퓨팅 자원을 효율적으로 사용하기 위한 방법으로,
**사용할 수 있는 자원보다 더 많은 양을 가상 머신이나 컨테이너에게 할당함으로써 전체 자원의
사용률을 높이는 방법이다.**

예를들어 하나의 노드에 두 개의 컨테이너(A, B)가 있는데 둘 다 메모리 제한량이 500mb라고 하자.
이때 B의 컨테이너가 항상 500mb가 가까이 사용하는 대신 A컨테이너는 100mb밖에 안쓴다고 할 떄
A의 사용량을 B가 더 사용하면 좋을 수 있다.
이때 B의 사용량을 최소로 보장해주는 것이 Requests이고 제한하는 것이 Limits이다.

위에서 만든 Yaml파일을 수정해본다.

> vi resource-limit-with-request-pod.yaml

apiVersion: v1
kind: Pod
metadata: 
  name: resource-limit-with-request-pod
  labels:
    name: resource-limit-pod
spec:
  containers:
  - name: nginx
    image: nginx:latest
  resources:
    limits:
      memory: "256Mi"
      cpu: "1000m"
    requests:
      memory: "128Mi"
      cpu: "500m"

cpu또한 같은 원리이다. 
'최소한 0.5 CPU만큼은 사용할 수 있지만 유휴 CPU자원이 있다면 최대 1CPU까지 사용할 수 있다.'
라고 할 수 있다.

단, requests는 컨테이너가 보장받아야 하는 최소한의 자원을 뜻하기 때문에 노드의 총 자원의
크기보다 더 많은 양의 requests를 할당할 수는 없다.
따라서 쿠버네티스의 스케줄러는 포드의 requests만큼 여유가 있는 노드를 선택해 포드를 생성한다.
```

그림 11.4 

- CPU 자원 사용량의 제한 원리
- QoS 클래스와 메모리 자원 사용량 제한 원리

```bash
포드의 컨테이너는 최대 Limists만큼의 자원을 사용할 수 있지만,
최소한 Requests만큼의 자원을 사용할 수 있도록 보장받는다.
이때 Requests보다 더 많은 자원을 사용하는 것을 오버커밋이라고 부르며,
Requests를 넘어서 자원을 사용하려 시도하다가 다른 컨테이너와 자원 사용이 충돌하게 되면
CPU 스로틀과 같은 원리에 의해 자원 사용이 실패할 수 있다.
이때 CPU같은 경우 스트롤이 걸릴 뿐 컨테이너 자체에는 큰 문제가 발생하지 않지만
메모리 사용량에 경합이 발생하면 문제가 심각해진다.
이러한 상황에서 쿠버네티스는 가용 메모리를 확보하기 위해 
**우선순위가 낮은 포드 또는 프로세스를 강제로 종료하도록 설계돼 있다.**
강제로 종료된 포드는 다른 노드로 옮겨가게 되는데, 
쿠버네티스에서는 이를 **퇴거(Evicition)**이라고 표현한다.

그렇다면 여기서 가장 중요한 부분은 
'노드에 메모리 자원이 부족해지면 어떤 포드나 프로세스가 먼저 종료돼야 하는가'이다.

이를 위해 쿠버네티스는 포드의 컨테이너에 설정된 Limits와 Requests의 값에 따라 내부적으로 
우선순위를 계산한다.
또한 쿠버네티스는 포드의 우선순위를 구분하기 위해 3가지 종류의 QoS(Quality Of Service)클래스를
명시적으로 포드에 설정한다.

*** 쿠버네티스에서의 메모리 부족과 OOM(Out Of Memory)**
쿠버네티스의 노드에는 각종 노드의 이상 상태 정보를 의미하는 Conditions라는 값이 존재한다.
이는 kubectl describe nodes 명령어로 확인할 수 있다.
이상 상태의 종류에는 MemoryPressure, DiskPressure 등이 있다. 

쿠버네티스 에이전트인 kubelet은 노드의 자원상태를 주기적으로 확인하면서
Conditions의 MemoryPressure,DiskPressure 등의 값을 갱신한다.

예를 들어 평소에 메모리가 부족하지 않을 때는 MemoryPressure의 값이 False로 설정돼 있다.
만약 메모리가 부족해지면 MemoryPressure 상태의 값이 True로 바뀐다.

> kubectl decribe nodes | grep -A9 Conditions

MemoryPressure는 기본적으로 노드의 가용메모리가 100Mi 이하일 때 발생하도록 
kubelet에 설정돼 있다. 발생하게되면 쿠버네티스는 해당 노드에서 실행 중이던
모든 포드에 대해 순위를 매긴 다음, 가장 우선순위가 낮은 포드를 다른 노드로 퇴거시킨다.
그뿐만 아니라 MemoryPressure의 값이 True인 노드에는 더 이상 포드를 할당하지 않는다.

만약 kubelet이 MemoryPressure상태를 감지하기 전에 급작스럽게 메모리 사용량이 많아질 경우,
리눅스 시스템의 OOM Killer라는 기능이 우선순위 점수가 낮은 컨테이너의 프로세스를 강제로 종료해
사용 가능한 메모리를 확보할 수도 있다.

OOM의 우선순위 점수에는 두 가지가 있는데, 
첫 번째는 oom_score_adj이고,
두 번째는 oom_score이다. 
OOM Killer는 oom_score의 값에 따라서 종료할 프로세스를 선정한다.

OOM Killer는 리눅스에 기본적으로 내장된 기능이기 때문에 아무것도 설정하지 않아도 
모든 프로세스에 자동으로 OOM 점수가 매겨진다.
OOM 점수가 높으면 높을수록 강제로 종료될 가능성이 커지기 때문에 절대로 종료되지 말아야하는
핵심 프로세스는 일반적으로 매우 낮은 값을 부여받는다.

예를 들어 쿠버네티스를 설치함으로써 실행되는 도커 데몬은 기본적으로 기본 OOM 점수가 -999로
설정돼 있다.
> ps aux | grep dockerd

> ls /proc/$프로세스ID

> cat /proc/$프로세스ID/oom_score_adj

*** QoS 클래스의 종류 - 1. Guaranteed 클래스**
이 클래스에는 3가지 종류가 있다. BestEffort, Burstable, Guaranteed 
포드의 자세한 정보를 출력하면 QoS클래스를 확인할 수 있다.

> kubectl describe pod resource-limit-pod | grep QoS

Guaranteed 클래스의 포드를 명시적으로 생성하고 싶다면 다음과 같이 YAML 파일에서 Limits의 
Requests를 동일하게 설정한 뒤 사용해도 된다.

....
  resources:
    limits:
      memory: "256Mi"
      cpu: "1000m"
    requests:
      memory: "256Mi"
      cpu: "1000m"

Guaranteed클래스로 분류되는 포드는 Requests와 Limits의 값이 동일하기 때문에 할당받은 자원을
아무런 문제 없이 사용할 수 있다.

즉, 자원의 오버커밋이 허용되지 않기 때문에 할당받은 자원의 사용을 안정적으로 보장(Guaranteed)
받을 수 있다고 생각하면 된다.

Guaranteed 클래스의 포드 내부에서 실행되는 프로세스들은 모두 기본 OOM 점수가 -998로 설정된다.
도커데몬이나 kubelet의 프로세스와 거의 동일한 레벨로 프로세스가 보호받기 때문에 노드에서
메모리가 고갈되더라도 시스템 컴포넌트가 요구하지 않는 한 Guaranteed 클래스의 포드나 프로세스가
강제로 종료되는 일은 없을 것이다.

*** QoS 클래스의 종류 - 2. BestEffort 클래스**
BestEffort는 Requests와 Limits를 아예 설정하지 않은 포드에 설정되는 클래스이다. 
즉, 포드의 스펙을 정의하는 yaml파일에서 resources 항목을 아예 사용하지 않으면 자동으로
이 클래스로 분류된다.
따라서 이번 장 이전에 생성했던 포드들은 모두 BestEffort클래스에 속한다.

> vi nginx-besteffort-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: nginx-besteffort-pod
spec: 
  containers:
  - name: nginx-besteffort-pod
    image: nginx:latest

BestEffort클래스의 포드는 Limits값을 설정하지 않았기 때문에 노드에 유휴 자원이 있다면
제한없이 모든 자원을 사용할 수 있다. 그렇지만 Requests 또한 설정하지 않았기 때문에
BestEffort클래스의 포드는 사용을 보장받을 수 있는 자원이 존재하지 않는다.
따라서 때에 따라서는 노드에 존재하는 모든 자원을 사용할 수도 있지만,
자원을 전혀 사용하지 못할 수도 있다.

*** QoS 클래스의 종류 - 3. Burstable 클래스**
이 클래스는 Requests와 Limits가 설정돼 있지만, Limits의 값이 Requests보다 큰 포드를 의미한다.
따라서 Burstable 클래스의 포드는 Requests에 지정된 자원만큼 사용을 보장받을 수 있지만,
상황에 따라서는 Limits까지 자원을 사용할 수도 있다.
Burstable 이라는 이름의 의미하는 것처럼 필요에 따라서 순간적으로 자원의 한계를 확장해
사용할 수 있는 포드이다.

Guaranteed나 BestEffort에 속하지 않는 모든 포드는 모두 Burstable클래스로 분류된다고 생각하면
쉽게 클래스의 종류를 구분할 수 있다. 다음과 같은 yaml은 Burstable클래스에 속한다.

> vi resource-limit-with-request-pod.yaml

...
  resources:
    limits:
      memory: "256Mi"
      cpu: "1000m"
    requests:
      memory: "128Mi"
      cpu: "500m"

Burstable 클래스의 포드는 주어진 Requests내에서 자원을 사용하면 문제가 없지만,
Requests를 넘어 Limits 범위 내에서 자원을 사용하려고 시도한다면 다른 포드와 자원 경합이 
발생할 수도 있다. 그러한 상황에서는 Requests보다 더 많은 자원을 사용하고 있는 포드나
프로세스의 우선순위가 더 낮게 설정된다.

*** QoS 클래스와 메모리 부족**
kubelet이 메모리가 부족한 상황을 감지하면 우선순위가 가장 낮은 포드를 종료한 뒤
다른 노드로 내쫓아내는 퇴거(Evict)를 수행한다.
만약 메모리 사용량이 갑작스럽게 높아지면 리눅스의 OOM Killer는 OOM점수가 가장 낮은
컨테이너의 프로세스를 강제로 종료할 수도 있다. 
포드가 다른 노드로 퇴거되면 단순히 다른 노드에서 포드가 다시 생성될 뿐이지만,
OOM Killer에 의해 포드 컨테이너의 프로세스가 종료되면 해당 컨테이너는 포드의 
재시작 정책(restartPolicy)에 의해 다시 시작된다.

기본적으로 포드의 우선순위는 Guaranteed가 가장 높고, Burstable, BestEffort순이다.
따라서 노드에 메모리가 부족하면 가장 먼저 BestEffort 포드가 종료되고 그 다음에는
Burstable포드가 종료된다. 
하지만 Burstable과 BestEffort클래스의 포드는 현재 메모리를 얼마나 사용하고 있는지에 따라서
우선순위가 역전될 수도 있기 때문이다.
**포드가 메모리를 많이 사용하면 사용할수록 우선순위가 낮아진다.**
```

- ResourceQuota와 LimitRange

```bash
여러 개발팀에게 쿠버네티스 환경을 제공하려면
네임스페이스를 개발팀마다 생성한 뒤, 각 개발팀이 해당 네임스페이스에서만 쿠버네티스의
API를 사용할 수 있도록 롤, 롤 바인딩 등으로 RBAC(Role-based Access Control)
를 설정하는 방식을 생각해 볼 수 있다. 하지만 특정 네임스페이스에서 포드에 자원을 과도하게
사용해 버리면 다른 네임스페이스에서는 자원이 부족한 상황이 발생할 수도 있다.
만약 쿠버네티스를 여러 사람 또는 개발팀이 함께 사용하고 있다면 각 네임스페이스에서 
할당할 수 있는 자원의 최대 한도 또는 범위를 설정할 필요가 있을 것이다.

이때 쿠버네티스에서는 ResourceQuota와 LimitRange라는 오브젝트를 이용해 자원 사용량을
관리할 수 있는 기능을 제공한다.

*** ResourceQuota로 자원사용량 제한**
ResourceQuota는 특정 네임스페이스에서 사용할 수 있는 자원 사용량의 합을 제한할 수 있는
쿠버네티스 오브젝트이다. ResourceQuota의 기능은 다음과 같다.

- 네임스페이스에서 할당할 수 있는 자원
(CPU, 메모리, 퍼시스턴트 볼륨 클레임의 크기, 컨테이너 내부의 임시 스토리지)의 총합을 제한한다.
- 네임스페이스에서 생성할 수 있는 리소스(서비스, 디플로이먼트 등)의 개수를 제한할 수 있다.

첫 번째 기능은 위의 예시에서 자원 사용량을 제한하기 위해서이고
두 번째 기능은 쿠버네티스 클러스터의 자원 고갈을 막기 위해서이다.
예를 들어 쿠버네티스 리소스 개수에 제한이 없이 끊임없이 생성한다고 가정할 때 
메모리나 스토리지가 꽉 찰 때까지 리소스가 계속 생성될 것이고, 결국 쿠버네티스
클러스터가 느려져 장애가 발생할 수 있다.
ResourceQuota는 이러한 상황을 방지하기 위해 네임ㅅ페이스에서 생성할 수 있는 
서비스, 디플로이먼트, 컨피그맵 등의 리소스의 개수를 제한하는 기능을 제공한다.

ResourceQuota는 네임스페이스에 종속되는 오브젝트이기 때문에 네임스페이스별로
ResourceQuota 리소스를 생성해야 한다. 기본적으로 어떠한 ResourceQuota도 생성돼 있지 않는다.
> kubectl get quota
> kubectl get resourcequota

ResourceQuota는 PVC나 ephemeral-storage의 크기를 제한하기 위해서 사용할 수도 있으나,
여기서는 default 네임스페이스의 CPU와 메모리의 Requests, Limits를 제한하는 ResourceQuota를
생성해 본다.

> vi resource-quota.yaml

apiVersion: v1
kind: ResourceQuota
metadata:
  name: resource-quota-example
  namespace: default
spec:
  hard:
    requests.cpu: "1000m"
    requests.memory: "500Mi"
    limits.cpu: "1500m"
    limits.memory: "1000Mi"

그렇다고 반드시 Requests와 Limits를 함께 제한할 필요는 없고,
CPU와 메모리를 하나의 ResourceQuota에서 제한할 필요도 없다.

위 yaml파일은 default 네임스페이스에서 사용할 수 있는 총 자원의 할당량을 제한할 것이다.

> kubectl apply -f resource-quota.yaml

> kubectl describe quota

ResourceQuota의 정보에는 현재 default 네임스페이스에 생성된 포드들의 자원 할당량의 합이 출력된다.
새롭게 생성되는 포드가 한계치보다 더 많은 자원을 사용하려고 하면 포드를 생성하는 API요청은 실패한다.

더 많은 메모리를 요구하는 포드를 생성하면 실패하지만 디플로이먼틑 통해 포드를 생성하면
디플로이먼트는 아무런 에러 없이 정상적으로 생성된다.
하지만 포드목록을 보면 포드는 생성되지 않는다.
이런 이유는 포드를 생성하는 주체는 디플로이먼트가 아니라 레플리카셋이라는 점이다.

디플로이먼트는 포드를 생성하기 위한 레플리카셋의 메타데이터를 선언적으로 가지고 있을 뿐,
디플로이먼트 리소스가 직접 포드를 생성하지는 않기 때문이다.

*** ResourceQuota로 리소스 개수 제한하기**
ResourceQuota는 자원의 사용량뿐만 아니라 디플로이먼트, 포드, 시크릿 등의 리소스 개수를
제한할 수 있다. ResourceQuota는 아래의 쿠버네티스 오브젝트의 개수를 제한한다.
- 디플로이먼트, 포드, 서비스, 시크릿, 컨피그맵, 퍼시스턴트 볼륨 클레임 등의 개수
- NodePort타입의 서비스 개수, LoadBalancer 타입의 서비스 개수
- QoS 클래스 중에서 BestEffort 클래스에 속하는 포드의 개수

포드나 서비스의 최대 개수를 제한하려면 yaml파일에 count/pod와 같은 형식으로 정의한다.
> vi quota-limit-pod-svc.yaml

apiVersion: v1
kind: ResourceQuota
metadata:
  name: resource-quota-example
  namespace: default
spec:
  hard:
    requests.cpu: "1000m"
    requests.memory: "500Mi"
    limits.cpu: "1500m"
    limits.memory: "1000Mi"
    count/pods: 3
    count/service: 5

위의 yaml파일에서 default네임스페이스에서는 최대 포드 3개, 서비스 5개를 생성할 수 있고,
그 이상 리소스 생성하려고 하면 API 요청이 거절한다.

*** ResourceQuota로 BestEffort 클래스의 포드 개수 제한하기**
ㅇㅇ
```
