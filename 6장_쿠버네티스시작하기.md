# 6장. 쿠버네티스 시작하기

- 모든 리소스는 오브젝트 형태로 관리

```bash
도커 스웜에서는 컨테이너 묶음을 서비스. 이것도 일종의 오브젝트
쿠버네티스에서는 컨테이너의 집합을 pods,
컨테이너 집합 관리하는 컨트롤러를 Replica Set,
사용자는 Service Account, 노드Node 도 모두 하나의 오브젝트이다.

사용할 수 있는 오브젝트확인 명령어
> kubectl api-resources
```

- YAML파일을 많이 사용

```bash
도커 스웜 모드에서는 docker service create~ 명령어 사용했다면
쿠버네티스는 kubectl 이라는 명령어 사용

스웜 모드에서 스택을 생성하기 위해 YAML 파일을 사용했던 것처럼
쿠버도 yaml을 사용한다.

쿠버를 잘 사용하는 것은 'yaml파일을 잘 작성하는 것'
```

- 쿠버네티스 여러 개의 컴포넌트로 구성

```bash
마스터 노드: api서버, 컨트롤러 매니저, 스케줄러, dns서버

모든 노드에서는 오버레이 네트워크를 구성을 위해 프락시, 
네트워크 플러그인이 실행

쿠버네티스 입장에서 도커 데몬 또한 하나의 컴포넌트이다.
컨테이너 사용을 위해 쿠버네티스가 도커를 이용하느느 방식

kubelet이란 에이전트가 모든 노드에서 기본적으로 실행되며, 
마스터 노드에는 api서버 등이 컨테이너로 실행된다.
```

## 6.2 포드: 컨테이너를 다루는 기본 단위

쿠버의 주요 오브젝트: 포드 레플리카셋 서비스 디플로이먼트

- 포드 사용

```bash
포드는 1개 이상의 컨테이너로 구성된 컨테이너의 집합

도커 엔진에서는 도커 컨테이너
스웜에서는 여러 개의 컨테이너로 구성된 서비스 
쿠버는 컨테이너 애플리케이션을 배포하기 위한 포드
1개의 포드는 1개의 컨테이너일수도 있고 여러개일 수도 있다.
```

- Nginx 컨테이너 포드 생성

```bash
> vi chap6/nginx-pod.yaml

apiVersion: v1
kind: Pod
metadata:
 name: my-nginx-pod
spec:
 containers:
 - name: my-nginx-container
   image: nginx:latest
   ports:
   - containerPort: 80
     protocol: TCP

> kubectl api-resources

작성한 yaml파일로 쿠버네티스 생성
> kubectl apply -f nginx-pod.yaml

특정 오브젝트 확인
> kubectl get pods

포드 컨테이너 내부 들어가기 
> kubectl exec -it my-nginx-pod bash

포드 로그 확인하기
> kubectl logs my-nginx-pod

쿠버 포드 삭제하기
> kubectl delete -f nginx-pod.yaml

```

- 포드 vs 도커 컨테이너

```bash
왜 포드라는 새로운 개념 사용?
=> 주요한 이유 중 하나는 여러 리눅스 네임스페이스를 공유하는 여러 
컨테이너들을 추상화된 집합으로 사용하기 위해서이다.

* 리눅스네임스페이스란
프로세스를 실행할 때 시스템의 리소스를 분리해서 실행할 수 있도록
도와주는 기능이다. 한 시스템의 프로세스들은 기본적으로 시스템의
리소스를 공유해서 실행한다.

포드 내부의 컨테이너들은 네트워크와 같은 리눅스 네임스페이스를 
공유한다.
```

- 완전한 애플리케이션으로서의 포드

```bash
'하나의 포드는 하나의 완전한 애플리케이션이다'

만약 두 개의 컨테이너를 하나의 포드에서 사용할 경우
주 컨테이너를 두고 기능 확장을 위한 추가 컨테이너를 함꼐 포드에
포함시킬 수 있다.

이렇게 정의된 추가적인 컨테이너를 사이드카 컨테이너라고 한다.
사이드카 컨테이너는 포드 내의 다른 컨테이너와 네트워크 환경 등을 공유한다.
따라서 여러 개의 컨테이너는 하나의 완전한 애플리케이션으로 동작한다.
```

## 6.3 레플리카셋: 일정 개수의 포드를 유지하는 컨트롤러

- 레플리카셋 사용 이유

```bash
yaml파일로 포드를 생성하고 지우면 해당 포드는 오직 
쿠버네티스 사용자만 관리할 수 있다.

yaml파일에 포드를 정의해 사용하는 방식은 여러가지 문제가 있음
따라서 쿠버네티스에서 포드만 정의해 사용하는 경우는 없고,
레플리카셋이라는 오브젝트를 함께 사용한다.

역할은 다음과 같다.
* 정해진 수의 동일한 포드가 항상 실행되도록 관리
* 노드 장애 등의 이유로 포드를 사용할 수 없다면 
 다른 노드에서 포드를 다시 생성
```

- 레플리카셋 사용하기

```bash
nginx 포드를 생성하는 레플리카셋 만들기

vi chap6/replicaset-nginx.yaml

apiVersion: apps/v1
kind: ReplicaSet
metadata:
 name: replicaset-nginx
spec:
 replicas: 3
 selector:
  matchLabels:
   app: my-nginx-pods-label
 template:
  metadata:
   name: my-nginx-pod
   labels:
    app: my-nginx-pods-label
   spec:
    containers:
    - name: nginx
      image: nginx:latest
      ports:
      - containerPort: 80

* spec.replicas: 동일한 포드를 몇 개 유지시킬 것인가 설정
* spec.template 아래의 내용: 포드를 생성할 때 사용할 템플릿 설정
 포드를 생성할 때 사용했던 yaml과 비슷하게 어떠한 포드를 어떻게 
 생성할 것인지 명시
 이를 보통 포드 스펙, 포드 템플릿이라고 함

레플리카셋 설정
> kubectl apply -f replicaset-nginx.yaml

포드 목록 확인
> kubectl get po

레플리카셋 목록 확인
> kubectl get rs

레플리카셋에서 관리하는 포드 늘리기
yaml파일에서 replicas: 4로 바꾸기
```

- 레플리카셋의 동작 원리

```bash
레플리카셋을 생성하면 포드가 생성되고, 레플리카셋을 삭제하면 포드 또한 삭제돼서
포드와 레플리카셋은 연결되어 있는 것처럼 보이지만 그렇지 않다.

오히려 포드와 레플리카셋은 느슨한 연결을 유지하고 있고 이는 라벨 셀렉터를 정의함으로써
이루어진다.

yaml파일에서 라벨은 쿠버네티스 리소스를 분류할 때 유용하게 사용할 수 있는 메타데이터이다.
예를 들어 레플리카셋은 spec.selector.matchLabel에 정의된 라벨을 통해 생성해야 하는 포드를
찾는다. 즉, app: my-nginx-pods-label 라벨을 가지는 포드의 개수가 replicas항목에
정의된 3개와 맞지 않으면 포드가 3이 되도록 생성하고 이미 3이라면 포드를 더 만들지 않는다.

따라서 레플리카셋의 목적은 'yaml파일에 정의된 일정 개수의 포드를 유지하는 것'이다.
```

## 6.4 디플로이먼트(Deployment): 레플리카셋, 포드의 배포를 관리

- 디플로이먼트 사용하기

```bash
실제 운영 환경에서는 레플리카셋을 yaml파일에서 사용하는 경우는 없다.
대부분 레플리카셋과 포드의 정보를 정의하는 디플로이먼트라는 이름의 오브젝트를 yaml파일에
정의해서 사용한다.

디플로이먼트는 레플리카셋의 상위 오브젝트이기 때문에 디플로이먼트를 생성하면 
해당 디플로이먼트에 대응하는 레플리카셋도 함께 생성된다.

"따라서 디플로이먼트를 사용하면 포드와 레플리카셋을 직접 생성할 필요가 없다."

* 디플로이먼트 생성하기
vi chapter6/deployment-nginx.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-nginx-deployment
spec:
  replicas: 3
  selector:
    metchLabels:
      app: my-nginx
    template:
      metadata:
        name: my-nginx-pod
        labels:
          app: my-nginx
      spec:
        containers:
        - name: nginx
          image: nginx:1.10
          ports:
          - containerPort: 80

위의 야믈파일에서 kind항목이 Deployment로만 바뀌었다.

* 실행
> kubectl apply -f deployment-nginx.yaml

* 목록 출력
> kubectl get deployment

* 레플리카셋 출력
> kubectl get replicasets

* 포드 출력
> kubectl get pods

디플로이먼트를 삭제하면 레플리카셋과 포드도 함께 삭제된다.
* 삭제
> kubectl delete deploy my-nginx-deployment
```

- 디플로이먼트를 사용하는 이유

```bash
왜 레플리카셋을 사용하지 않고 더 상위 개념이 디플로이먼트 사용할까?

핵심적인 이유 중 하나는 "애플리케이션의 업데이트와 배포를 더욱 편하게 하기 위해서이다."
디플로이먼트는 컨테이너 애플리케이션을 배포하고 관리하는 역할을 담당한다.
예를 들면 애플리케이션 업데이트할 때 레플리카셋의 변경 사항을 저장하는 리비전을 남겨서
롤백하게 하거나
무중단 서비스를 위해 포드의 롤링 업데이트의 전략을 지정할 수 있다.

이전과 동일하게 야믈파일로 deployment-nginx.yaml파일로 디플로이먼트를 생성하되
--record라는 특수한 옵션을 추가한다.

> kubectl apply -f deployment-nginx.yaml --record
> kubectl get pods

이때 애플리케이션의 버전이 업데이트되어 포드의 이미지를 변경해야 한다고 한다면 
아래와 같은 명령어를 실행하면 된다.
> kubectl set image deployment my-nginx-deployment nginx=nginx:1.11 --record

* 새롭게 생성된 포드 목록 확인
> kubectl get pods

* 레플리카셋 목록 확인
> kubectl get replicasets

이때 두 개의 레플리카셋이 있는데 replicas값이 0인 것이 첫 번째로 만든 레플리카셋이고
3이 이미지 변경하고나서 생긴 레플리카셋이다.

이렇게[ 디플로이먼트는 새로운 레플리카셋과 포드를 만들어도 기존의 오브젝트들은
남겨놓음으로써 롤백할 수 있는 리비전을 남겨준다.
리비전 정보를 자세히 보는 명령어는 아래와 같다.
> kubectl rollout history deployment my-nginx-deployment

--record=true 옵션으로 디플로이먼트를 변경하면 변경사항을 위와 같이 디플로이먼트에 기록한다.

* 이전 리비전으로 이동하는 롤백 명령어
--to-revision에는 되돌리려는 리비전의 번호를 입력한다.
> kubectl rollout undo deployment my-nginx-deployment --to-revision=1
> kubectl get replicasets

* 쿠버네티스 리소스 자세히 보기
> kubectl describe deploy my-nginx-deployment

이처럼 "디플로이먼트는 여러 개의 레플리카셋을 관리하기 위한 상위 오브젝트이다."
디플로이먼트를 사용하면 레플리카셋의 리비전 관리뿐만 아니라 다양한 포드의 롤링 업데이트를
할 수 있다. "따라서 디플로이먼트 사용을 권장한다."

* 리소스 모두 정리하는 명령어
> kubectl delete deployment,pod,rs --all
```

## 6.5 서비스(Service): 포드를 연결하고 외부에 노출

```bash
디플로이먼트를 통해 생성된 포드에 접근하는 방법

디플로이먼트의 포드를 외부에 노출시키려면 yaml파일에서 containPort옵션으로 특정 포트를
열러주고 서비스(Service)라는 쿠버네티스 오브젝트를 생성해야 한다.

서비스는 포드에 접근하기 위한 규칙을 정의하기 때문에 쿠버네티스에서 애플리케이션을
배포하기 위해서는 반드시 알아야 할 오브젝트이다.

* 서비스의 주요 기능
- 여러 개의 포드에 쉽게 접근하도록 도메인 이름 부여
- 여러 개의 포드에 접근할 때 요청을 분산하는 로드 밸런서 기능 수행
- 클라우드 플랫폼의 로드 밸런서, 클러스터 노드의 포트 등을 통해 포드를 외부에 노출
```

- 서비스의 종류

```bash
서비스를 생성하기에 앞서 yaml파일을 이용해 디플로이먼트를 먼저 생성한다.

> gedit chapter6/deployment-hostname.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: hostname-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webserver
  template:
    metadata:
      name: my-webserver
      labels:
        app: webserver
    spec:
      containers:
      - name: my-webserver
        image: alicek106/rr-test:echo-hostname
        ports:
        - containerPort: 80

> kubectl apply -f deployment-hostname.yaml

* 포드의 IP주소 확인
> kubectl get pods
> kubectl get pods -o wide

* crul로 HTTP요청보내서 포드의 이름 확인
>kubectl run -i --tty --rm debug \
--image=alicek106/ubuntu:curl --restart=Never curl $IP주소 | grep Hello

```

- ClusterIP 타입의 서비스 - 쿠버네티스 내부에서만 포드 접근

```java
> vi chapter6/hostname-svc-clusterip.yaml

apiVersion: v1
kind: Service
metadata:
  name: hostname-svc-clusterip
spec:
  ports:
    - name: web-port
      port: 8080
      targetPort: 80
    selector:
      app: webserver
    type: ClusterIP

* spec.selector: selector항목은 이 서비스에서 어떠한 라벨을 가지는 포드에 접근가능하게 
할 것인지를 결정한다. 위에서는 app: webserver라는 라벨을 가지는 포드들에 접근할 수 있도록
서비스를 생성했다.

* spec.ports.port: 서비스의 IP에 접근할 때 사용할 포트 설정

* spec.ports.targetPort: selector항목에서 작성한 라벨에 의해 접근가능하게 된 포드들이
내부적으로 사용하고 있는 포트를 입력

* spec.type: 이 서비스가 어떤 타입인지 나타낸다. 
종류로는 ClusterIP, NodePort, LoadBalancer 등이 있다.

#서비스 생성
> kubectl apply -f hostname-svc-clusterip.yaml

#서비스 목록 확인
> kubectl get services

#임시 포드를 만들어 만든 서비스에 요청 전송해보기
> kubectl run -i --tty --rm debug --image=alicek106/ubuntu:curl --restart=Never -- bash
> curl 10.101.98.33:8080 --silent | grep Hello
> curl 10.101.98.33:8080 --silent | grep Hello
> curl 10.101.98.33:8080 --silent | grep Hello

결과를 보면 서비스의 IP와 포트를 통해 포드에 접근할 수 있고, 서비스와 연결된 여러 개의 포드에
자동으로 요청이 분산되고 있다. 서비스를 생성할 때 별도의 설정을 하지 않아도 서비스는 연결된
포드에 대해 로드밸런싱을 수행한다.

또한 서비스IP뿐만 아니라 서비스 이름으로도 접근할 수 있다.
> curl hostname-svc-clusterip:8080 --silent | grep Hello
```

- NodePort타입의서비스 - 서비스이용해서 포드 외부에 노출

```java
ClusterIP 타입의 서비스는 내부에서만 접근 가능하지만 NodePort 타입의 서비스는 
클러스터 외부에서도 접근할 수 있다.
NodePort타입의 서비스는 모든 노드의 특정 포트를 개방해 서비스에 접근하는 방식이다. 
스웜 모드에서 컨테이너를 외부로 노출하는 방식과 비슷하다.

# NodePort타입의 서비스를 생성하기 위한 YAML파일을 작성한다.
> vi chapter6/hostname-svc-nodeport.yaml

apiVersion: v1
kind: Service
metadata:
  name: hostname-svc-nodeport
spec:
  ports:
    - name: web-port
      port: 8080
      targetPort: 80
  selector:
    app: webserver
  type: NodePort

ClusterIP 타입의 서비스를 생성했을 때와 비교해보면 type항목을 NodePort로 
설정한 점 빼고는 동일하다.

# NodePort타입 서비스 생성
> kubectl apply -f hostname-svc-nodeport.yaml
> kubuctl get services
서비스 목록에서 PORT(s)항목에 출력된 31514숫자는 모든 노드에서 동일하게 
접근할 수 있는 포트를 의미한다.
이때 각 노드에서 개방되는 포트는 기본적으로 30000~32768 포트 중에서 랜덤으로 
선택되지만, YAML파일에서 임의로 포트를 지정할 수 있다.
spec:ports:nodePort: 31000

즉, 클러스터의 모든 노드에 내부 IP 또는 외부 IP를 통해 31514포트로 접근하면 동일한 
서비스에 연결할 수 있다.

# 각 노드의 내부 IP 확인하기
> kubectl get nodes -o wide

# 각 노드의 내부 IP의 31514포트로 curl명령어 실행
> curl $노드1_내부IP:31514 --silent | grep Hello
> curl $노드2_내부IP:31514 --silent | grep Hello
> curl $노드3_내부IP:31514 --silent | grep Hello

클라우드를 사용할 경우 Security Group에서 별도의 Inbound규칙에 해당 포트를 
추가해야한다.

이때 신기한 점은 서비스 목록을 출력했을 때 CLUSTER-IP 항목에 내부 IP가 할당된 것을 
알 수 있다.
이는 NodePort타입의 서비스가 ClusterIP으 기능을 포함하고 있기 때문이다.
NodePort타입의 서비스를 생성하면 자동으로 ClusterIP 기능을 사용할 수 있기 때문에
서비스의 내부 IP와 DNS이름을 사용해서 접근할 수 있다.

하지만 실제 운영환경에서 NodePort로 서비스를 외부에 제공하지 않는다.
그 이유는 포트번호를 80 또는 443으로 설정하기 적절치 않으며, SSL 인증서 적용, 
라우팅 등과 같은
복잡한 설정을 서비스에 적용하기가 어렵기 때문이다.
그래서 보통 인그레스(Ingress)라고 부르는 쿠버네티스의 오브젝트에서 간접적으로 
사용하는 경우가 많다.

```

- 클라우드 플랫폼의 로드밸런서와 연동 - LoadBalancer타입의 서비스

```java
LoadBalancer 타입의 서비스는 서비스 생성과 동시에 로드 밸런서를 새롭게 생성해서
포드와 연결한다. NodePort 서비스에서는 각 노드의 IP를 알아야 포드에 접근할 수 
있었지만 LoadBalancer타입의 서비스는 클라우드 플랫폼으로부터 도메인 이름과 IP
를 할당받기 때문에 NodePort보다 더욱 쉽게 포드에 접근할 수 있다.

단 LoadBalancer 서비스는 로드 밸런서를 동적으로 생성하는 기능을 제공하는 환경에서만
사용가능하다. AWS, GCP 등과 같은 클라우드 플랫폼 환경에서만 사용할 수 있다.

위에서 사용했던 hostname-deployment 디플로이먼트가 미리 생성돼 있다고 가정하고,
디플로이먼트의 포드에 서비스를 연결해본다.

# YAML파일 작성
> vi chaper6/hostname-svc-lb.yaml

apiVersion: v1
kind: Service
metadata:
  name: hostname-svc-lb
spec:
  ports:
    - name: web-port
      port: 80
      targetPort: 80
  selector:
    app: webserver
  type: LoadBalancer

여기서 ports.port항목은 로드밸런서에 접근하기 위한 포트를 의미한다.

# 서비스 생성
> kubectl apply -f hostname-svc-lb.yaml
> kubectl get svc

LoadBalncer 타입 또한 NodePort나 ClusterIP와 동이랗게 서비스의 CLUSTER-IP가
할당됐으며, 포드에서는 서비스의 IP 또는 서비스 이름으로 서비스에 접근할 수 있다.
여기서 눈여겨봐야 할 것은 EXTERNAL-IP 항목이다. 
이 주소는 클라우드 플랫폼인 AWS로부터 자동으로 할당된 것이며, 
이 주소와 80포트(YAML파일의 ports.port)를 통해 포드에 접근할 수 있다.

> curl $EXTERNAL-IP1 --silent | grep Hello
> curl $EXTERNAL-IP2 --silent | grep Hello

서비스 목록명령어에서 PORT(s)항목의 32620은 각 노드에서 동일하게 접근할 수 있는
포트번호를 말한다.
32620이 포트를 이용해서 각 노드의 IP로 접근해보면 로드 밸런서와 똑같이 포드에
접근할 수 있다.

> curl $노드1IP:32620 --silent | grep Hello
> curl $노드2IP:32620 --silent | grep Hello
> curl $노드3IP:32620 --silent | grep Hello

AWS 로드밸런서 화면을 보면 모든 워커 노드가 로드밸런서에 연결돼 있음을 알 수 있다.
로드 밸런서로 들어온 요청은 각 워커의 32620 포트로 전달되고 있다.

즉, 위에서 LoadBalancer타입을 명시해 서비스를 생성했지만, NodePort의 간접적인
기능 또한 자동으로 사용할 수 있다.

또한 AWS로드 밸런서 관리화면에서 '유형'을 보면 'classic'으로 되어 있다.
쿠버네티스 1.18.0버전 기준으로 서비스의 YAML파일에서 아무런 설정하지 않으면
AWS의 클래식 로드밸런서를 생성한다.

이때 원한다면 NLB(네트워크 로드 밸런서)를 생성할 수도 있다.
YAML파일에 다음과 같이 적어주면 된다.
> vi chapter6/hostname-svc-nlb.yaml

apiVersion: v1
kind: Service
metadata:
  name: hostname-svc-nlb
  annotations:
  service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
spec:
  ports:
    - name: web-port
      port: 80
      targetPort: 80
  selector:
    app: webserver
  type: LoadBalancer

로드밸런서의 사용을 마쳤다면 서비스를 삭제한다. 서비스 삭제하면 AWS에서 생성된
로드밸런서도 같이 삭제된다.
> kubectl delete -f hostname-svc-lb.yaml

```

- 트래픽 분배를 결정하는 서비스 속성: externalTrafficPolicy

```java
LoadBalancer타입의 서비스를 사용하면 외부로부터 들어온 요청은 각 노드 중 하나로 보내지며,
그 노드에서 다시 포드 중 하나로 전달된다. NodePort타입도 마찬가지로 각 노드로 들어오는 요청은 
다시 포드 중 하나로 전달된다.

이때 효율적이지 않은 경우도 발생한다. 예를 들어 A노드로 들어오는 요청은 A노드에서 처리할 수 있지만
이를 B포드로 전달되어 처리하면 불필요한 네트워크 홉이 발생하게 된다.

이러한 요청 전달 메커니즘은 서비스의 속성 중 externalTrafficPolicy 항목에 정의돼 있다.
> kubectl get svc hosname-svc-nodeport -o yaml
-o 옵션을 주면 리소스의 정보를 yaml, json 등의 형식으로 출력할 수 있다.
이때 위 명령어의 결과를 보면 externalTrafficPolicy가 Cluster로 되어 있는 것을 확인할 수 있다.

여기서 externalTrafficPolicy를 Local로 설정하면 포드가 생성된 노드에서만 포드로 접근할 수 있고,
로컬 노드에 위치한 포드 중 하나로 요청이 전달된다.
이렇게 되면 네트워크 홉이 발생하지 않고, 전달되는 요청의 클라이언트 IP도 보존된다.

> vi cpahter6/hostname-svc-lb-local.yaml

apiVersion: v1
kind: Service
metadata:
  name: hostname-svc-lb-local
spec:
  externalTrafficPolicy: Local
  ports:
    - name: web-port
      port: 80
      targetPort: 80
  selector:
    app: webserver
  type: LoadBalancer

> kubectl apply -f hostname-svc-lb-local.yaml

위에서 생성한 hostname-deployment를 계속 사용하고 있다면 3개의 포드를 1개로 줄여 한 개의
노드에서만 포드가 존재하도록 변경한다.
> kubectl scale --replicas=1 deployment hostname-deployment

> kubectl get deploy

> kubectl get pods -o wide

#개방된 포트번호 확인
> kubectl get services

> curl $노드IP:포드포트번호 --silent | grep Hello

하지만 externalTrafficPolicy를 Local로 설정하는 것이 무조건 좋은 것은 아니다.
각 노드에 포드가 고르지 않게 스케줄링됐을 때, 요청이 고르게 분산되지 않을 수 있기 때문이다.

**따라서 둘 다 장단점이 있기 때문에 불필요한 네트워크 홉으로 인한 레이턴시나 클라이언트 IP보존이
중요하지 않다면 Cluster를, 그 반대라면 Local을 적절히 사용한다.**
```

- 요청을 외부로 리다이렉트하는 서비스 : ExternalName

```java
쿠버네티스를 외부 시스템과 연동해야 할 때는 ExternalName타입의 서비스를 사용할 수도 있다.
ExternalName 타입을 사용해 서비스를 생성하면 서비스가 외부 도메인을 가리키도록 설정할 수 있다.

**ExternalName 타입의 서비스는 쿠버네티스와 별개로 존재하는 레거시 시스템에 연동해야 하는 
상황에서 유용하게 사용할 수 있다.**

> vi chapter6/external-svc.yaml

apiVersion: v1
kind: Service
metadata:
  name: externalname-svc
spec:
  type: ExternalName
  externalName: my.database.com

* 리소스 정리
이번 장의 실습에서 사용한 리소스 정리하기
> cd kubenetes/chapter6
> kubectl delete -f ./
```
